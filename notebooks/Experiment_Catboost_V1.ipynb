{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Expérimentation V1 : Modèle Hybride CatBoost\n",
                "\n",
                "Ce notebook implémente la stratégie **Supervisé Multiclass + Smart Masking** définie dans `notes/nouveau_model.md`.\n",
                "\n",
                "### Objectifs\n",
                "1. Préparer un dataset d'entraînement via **Smart Masking** (sur-échantillonnage des produits rares).\n",
                "2. Entraîner un **CatBoostClassifier** qui utilise nativement les features catégorielles.\n",
                "3. Évaluer la performance brute vs **Hybride** (combinaison avec la Baseline).\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import sys\n",
                "from pathlib import Path\n",
                "import numpy as np\n",
                "import pandas as pd\n",
                "import matplotlib.pyplot as plt\n",
                "\n",
                "# Ajout du dossier src au path\n",
                "project_root = Path(\"..\").resolve()\n",
                "if str(project_root) not in sys.path:\n",
                "    sys.path.append(str(project_root))\n",
                "\n",
                "from src.config import DATA_DIR, ARTIFACTS_DIR\n",
                "from src.models.catboost.dataset import CatboostDatasetBuilder\n",
                "from src.models.catboost.trainer import CatboostTrainer\n",
                "from src.models.catboost.predictor import HybridPredictor\n",
                "from src.pipelines.baseline_pipeline import BaselineArtifact\n",
                "from src.evaluation.evaluate_V2 import evaluate_masking"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Chargement et Nettoyage"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "train_path = DATA_DIR / \"Train.csv\"\n",
                "test_path = DATA_DIR / \"Test.csv\"\n",
                "\n",
                "df_train = pd.read_csv(train_path)\n",
                "df_test = pd.read_csv(test_path)\n",
                "\n",
                "# Définition des colonnes (même logique que Baseline)\n",
                "PROFILE_COLS = [\n",
                "    \"ID\",\"join_date\",\"sex\",\"marital_status\",\"birth_year\",\n",
                "    \"branch_code\",\"occupation_code\",\"occupation_category_code\"\n",
                "]\n",
                "PRODUCT_COLS = [c for c in df_train.columns if c not in PROFILE_COLS]\n",
                "\n",
                "print(f\"Produits ({len(PRODUCT_COLS)}): {PRODUCT_COLS}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def normalize_cat(s: pd.Series) -> pd.Series:\n",
                "    return s.astype(str).str.strip().str.casefold()\n",
                "\n",
                "def apply_cleaning(df: pd.DataFrame) -> pd.DataFrame:\n",
                "    out = df.copy()\n",
                "    # Normalisation des catégorielles\n",
                "    for c in [\"sex\", \"marital_status\", \"branch_code\", \"occupation_code\", \"occupation_category_code\"]:\n",
                "        out[c] = normalize_cat(out[c])\n",
                "    \n",
                "    # Join Year\n",
                "    jy = pd.to_numeric(out[\"join_date\"].astype(str).str.split(\"/\").str[-1], errors=\"coerce\")\n",
                "    out[\"join_year\"] = jy.fillna(2017) # Fillna moyenne/mode rapide\n",
                "    \n",
                "    # Age\n",
                "    out[\"age\"] = (out[\"join_year\"] - out[\"birth_year\"]).clip(18, 90).fillna(38)\n",
                "    \n",
                "    return out\n",
                "\n",
                "df_train_clean = apply_cleaning(df_train)\n",
                "df_test_clean = apply_cleaning(df_test)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Préparation du Dataset Supervisé\n",
                "\n",
                "Nous divisons le train en deux :\n",
                "- **Train Split** (80%) : Pour entraîner CatBoost avec Smart Masking.\n",
                "- **Val Split** (20%) : Pour évaluer la performance Hybride (produit masqué)."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from sklearn.model_selection import train_test_split\n",
                "\n",
                "# On garde uniquement les clients avec au moins 2 produits pour le 'val' (pour pouvoir en cacher 1)\n",
                "# Mais pour le train, on peut utiliser tout le monde (même taille 1 pour apprendre le profil? Non, besoin de target)\n",
                "# Le dataset builder filtre automatiquent min_basket=2\n",
                "\n",
                "ids_unique = df_train_clean[\"ID\"].unique()\n",
                "train_ids, val_ids = train_test_split(ids_unique, test_size=0.2, random_state=42)\n",
                "\n",
                "df_local_train = df_train_clean[df_train_clean[\"ID\"].isin(train_ids)].copy()\n",
                "df_local_val = df_train_clean[df_train_clean[\"ID\"].isin(val_ids)].copy()\n",
                "\n",
                "print(f\"Train local: {df_local_train.shape}, Val local: {df_local_val.shape}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Configuration du Builder\n",
                "CAT_FEATURES = [\"sex\", \"marital_status\", \"branch_code\", \"occupation_code\", \"occupation_category_code\"]\n",
                "\n",
                "builder = CatboostDatasetBuilder(\n",
                "    product_cols=PRODUCT_COLS,\n",
                "    cat_cols=CAT_FEATURES,\n",
                "    random_state=42\n",
                ")\n",
                "\n",
                "print(\"Construction du dataset train avec Smart Masking...\")\n",
                "X_train, y_train = builder.build_dataset(df_local_train, strategy='smart', min_basket=2)\n",
                "\n",
                "print(f\"X_train shape: {X_train.shape}\")\n",
                "print(X_train.head(3))"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Entraînement CatBoost"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Correction : passage des arguments directement sans dictionnaire 'params'\n",
                "trainer = CatboostTrainer(\n",
                "    cat_features=CAT_FEATURES,\n",
                "    iterations=500,\n",
                "    learning_rate=0.1,\n",
                "    depth=6\n",
                ")\n",
                "\n",
                "trainer.fit(X_train, y_train)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Évaluation Hybride (Validation)\n",
                "\n",
                "On charge la baseline pour l'hybridation."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Chargement Baseline Artifact\n",
                "baseline_art = BaselineArtifact.load(ARTIFACTS_DIR / \"baseline_v0\")\n",
                "\n",
                "# Création du Prédicteur Hybride\n",
                "hybrid_model = HybridPredictor(trainer, baseline_art)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Préparation des données de validation pour evaluate_masking\n",
                "# evaluate_masking attend X_full (produits) et utilise score_fn(x, idx)\n",
                "# Pour que score_fn accède au contexte, on a besoin d'accéder à df_local_val via idx\n",
                "\n",
                "# On doit s'assurer que les indices correspondent.\n",
                "df_local_val = df_local_val.reset_index(drop=True)\n",
                "X_val_full = df_local_val[PRODUCT_COLS].values\n",
                "\n",
                "# Fonction de scoring wrapper\n",
                "def hybrid_scorer(x_row_obs, idx_original):\n",
                "    # 1. Reconstruire le contexte (profil)\n",
                "    context_row = df_local_val.iloc[[idx_original]].copy()\n",
                "    \n",
                "    # 2. Mettre à jour les produits observés dans le contexte\n",
                "    # (car evaluate_masking a masqué un produit dans x_row_obs)\n",
                "    context_row[PRODUCT_COLS] = x_row_obs # Broadcast\n",
                "    \n",
                "    # 3. Prédiction (alpha=0.3 par exemple)\n",
                "    # predict_proba attend un DataFrame\n",
                "    probas = hybrid_model.predict_proba(context_row, alpha=0.3)\n",
                "    return probas[0]\n",
                "\n",
                "# Évaluation\n",
                "print(\"Évaluation sur le set de validation...\")\n",
                "info, metrics = evaluate_masking(\n",
                "    X_full=X_val_full, \n",
                "    score_fn=hybrid_scorer, \n",
                "    hide_k=1, \n",
                "    min_observed=1\n",
                ")\n",
                "\n",
                "print(\"\\n--- Résultats Hybride (Alpha=0.3) ---\")\n",
                "print(metrics)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. Analyse de l'Importance des Features"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "fi = trainer.model.get_feature_importance(prettified=True)\n",
                "print(fi.head(10))\n",
                "\n",
                "plt.figure(figsize=(10, 6))\n",
                "pl = fi.head(15)\n",
                "plt.barh(pl[\"Feature Id\"], pl[\"Importances\"])\n",
                "plt.gca().invert_yaxis()\n",
                "plt.title(\"Top 15 Feature Importance\")\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 6. Comparaison Finale : Baseline vs Hybride\n",
                "\n",
                "C'est le moment de vérité. On compare la Baseline pure (V0) contre notre nouveau modèle Hybride (V1) sur le même set de validation.\n",
                "On s'attend à ce que l'Hybride soit meilleur, surtout sur le Hit@1."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 1. Évaluer la Baseline Pure sur le MÊME set de validation\n",
                "print(\"Évaluation de la Baseline Pure...\")\n",
                "def baseline_scorer(x_row_obs, idx_original):\n",
                "    # Baseline ignore idx_original et contexte\n",
                "    return baseline_art.cond.score_one(x_row_obs)\n",
                "\n",
                "_, metrics_bl = evaluate_masking(\n",
                "    X_full=X_val_full, \n",
                "    score_fn=baseline_scorer, \n",
                "    hide_k=1, \n",
                "    min_observed=1\n",
                ")\n",
                "\n",
                "# 2. Comparaison\n",
                "comp_df = pd.DataFrame({\n",
                "    \"Baseline (V0)\": metrics_bl,\n",
                "    \"Hybride (V1)\": metrics\n",
                "})\n",
                "comp_df[\"Gain\"] = comp_df[\"Hybride (V1)\"] - comp_df[\"Baseline (V0)\"]\n",
                "\n",
                "print(\"\\n=== Comparaison des Performances ===\")\n",
                "print(comp_df.round(4))\n",
                "\n",
                "# Petit plot\n",
                "comp_df[[ \"Baseline (V0)\", \"Hybride (V1)\"]].plot(kind='bar', figsize=(10, 5))\n",
                "plt.title(\"Performance par Métrique : V0 vs V1\")\n",
                "plt.ylabel(\"Score\")\n",
                "plt.grid(axis='y', alpha=0.3)\n",
                "plt.show()"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "env",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.10.11"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}
